{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import scipy.optimize as opt\n",
    "import scipy as sci\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad, jacobian\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDN with Learnable parameters\n",
    "\n",
    "In this tutorial, we extends robust pooling to adaptive robust pooling; we introduce adaptive feature projections.\n",
    "\n",
    "Previously, we computed the gradient of robust pooling respect to input x: $Dy(x)$. The parameter $c$ is fixed during proprogation, thus there is no need compute its gradient. Now we relax this restriction, include $c$ as a trainable parameter and compute its gradient $Dy(c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-form Gradient  for adaptive robust pooling\n",
    "In this section we compute the closed-form gradient of 3 different robust pooling function: peseudo-Huber, Huber, Welsch.\n",
    "\n",
    "Recall the gradient for unconstrained function: \n",
    "\n",
    "$D(y(c)) = -H^{-1}B$ \n",
    "\n",
    "where $H= D^2_{YY}f(c, y)$ and $B= D^2_{c Y} f(c,y) $\n",
    "\n",
    "### Pseudo- Huber\n",
    "\n",
    "$y \\in  \\text{argmin}_y  \\sum_i^n c^2 (\\sqrt {1+ \\frac {(y-x_i)^2} {c^2}}-1)$\n",
    "\n",
    "\n",
    "###  Huber\n",
    "\n",
    "$y \\in \\text{argmin}_y \\sum_{i=1}^n $ $\\begin{cases} \n",
    "\\frac {1} {2} (y-x_i)^2 & \\text{$ |y-x_i| \\leq c $} \\\\\n",
    "\\alpha(|y-x_i|-\\frac {1} {2} c) & \\text{otherwise}\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "### Welsch\n",
    "\n",
    "$y \\in  \\text{argmin}_y  \\sum_{i=1}^n (1-exp(- \\frac {(y-x_i)^2} {2c^2}))$\n",
    "\n",
    "\n",
    "Below is the implmentation of $Dy(\\alpha)$, I adapted some previous code from DDN repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyc_closed_form(x,y,c,p='pseudo-huber'):\n",
    "    c_sq=c**2\n",
    "    if p=='pseudo-huber':\n",
    "        dyy = np.array([np.power(1.0 + np.power(y - xi, 2.0) / c_sq, -1.5) for xi in x])\n",
    "        dyc =  np.sum([np.power(y-xi,3)/(np.power(np.power((y-xi)/c,2)+1,1.5)*np.power(c,3)) for xi in x])\n",
    "    elif p=='huber':\n",
    "        dyy = np.array([1.0 if np.abs(y - xi) <= c else 0.0 for xi in x])\n",
    "        dyc = np.sum(np.array([0.0 if np.abs(y - xi) <= c else (1.0 if y-xi>0 else -1.0) for xi in x]))\n",
    "    else:\n",
    "        z = np.power(x - y, 2.0)\n",
    "        dyy = np.array([(c_sq - zi) / (c_sq * c_sq) * np.exp(-0.5 * zi / c_sq) for zi in z])\n",
    "        dyc=np.sum(np.array([-np.exp(-0.5 * np.power((y - xi)/c,2))*((2*(y-xi)*c_sq-np.power(y-xi,3))/(c**5)) for xi in x])) \n",
    "    return -1.0 * dyc/np.sum(dyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the gradient using pytorch autograd libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the objective function from ddn.basic.node\n",
    "def f(x, y, c,p='pseudo-huber'):\n",
    "    c_sq=c**2\n",
    "    if p=='pseudo-huber':\n",
    "        phi= lambda z: (c**2) * (np.sqrt(1.0 + np.power(z, 2.0) / (c**2)) - 1.0)\n",
    "    elif p=='huber':\n",
    "        phi = lambda z: np.where(np.abs(z) <= c, 0.5 * np.power(z, 2.0), c * np.abs(z) - 0.5 * c_sq)\n",
    "    elif p=='welsch':\n",
    "        phi = lambda z: 1.0 - np.exp(-0.5 * np.power(z, 2.0) / c_sq)\n",
    "    elif p=='trunc-quad':\n",
    "        phi = lambda z: np.minimum(0.5 * np.power(z, 2.0), 0.5 * c_sq)\n",
    "    return np.sum([phi(y - xi) for xi in x])\n",
    "\n",
    "# the solve objective function from ddn.basic.node\n",
    "def solve(x,c ,f, p='pseudo-huber'):\n",
    "    result = opt.minimize(lambda y : f(x, y, c,p), np.mean(x))\n",
    "    return result.x\n",
    "\n",
    "def dyc(x,y,c,p='pseudo-huber'):\n",
    "    fY = grad(f, 1)\n",
    "    fYY = jacobian(fY, 1)\n",
    "    fCY = jacobian(fY, 2)\n",
    "    return -1.0 * np.linalg.pinv(fYY(x, y, c,p)).dot(fCY(x, y, c,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the correctness of closed form graident by comparing it with autograd gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10 # number of input points\n",
    "y_target = np.array([0.0])\n",
    "x_init = np.random.rand(n)\n",
    "# add an outlier\n",
    "x_init[np.random.randint(len(x_init))] += 100.0 * np.random.rand(1)\n",
    "x_init=np.array([ 1.4748, -0.0034,  2.1072, -0.0675, -0.7821, -0.9080, -2.0427,\n",
    "          -1.9460,  1.7862,  0.1601])\n",
    "c_init = random.uniform(0.1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 1.4748 -0.0034  2.1072 -0.0675 -0.7821 -0.908  -2.0427 -1.946   1.7862\n",
      "  0.1601]\n",
      "c: 6.3957496996326855\n",
      "error between autograd and closed-form:\n",
      "pseudo-huber  [2.92734587e-18]\n",
      "huber  [0.]\n",
      "welsch  [1.08420217e-18]\n"
     ]
    }
   ],
   "source": [
    "print('x:',x_init)\n",
    "print('c:',c_init)\n",
    "#valid the analyic gradient is the same as autograd solution\n",
    "print(\"error between autograd and closed-form:\")\n",
    "y_init = solve(x_init,c_init,f,'pseudo-huber')\n",
    "print(\"pseudo-huber \",abs(dyc_closed_form(x_init,y_init,c_init,'pseudo-huber')-dyc(x_init,y_init,c_init,'pseudo-huber')))\n",
    "y_init = solve(x_init,c_init,f,'huber')\n",
    "print(\"huber \",abs(dyc_closed_form(x_init,y_init,c_init,'huber')-dyc(x_init,y_init,c_init,'huber')))\n",
    "y_init = solve(x_init,c_init,f,'welsch')\n",
    "print(\"welsch \",abs(dyc_closed_form(x_init,y_init,c_init,'welsch')-dyc(x_init,y_init,c_init,'welsch')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-form Gradient  for adaptive sphere and ball projections\n",
    "Similar to robust pooling, we can compute the gradient respect to ball projection. \n",
    "\n",
    "\n",
    "The adaptive sphere and ball projection problems in general is defined:\n",
    "\n",
    "\\begin{array}{lll}\n",
    "    y \\in & \\text{argmin}_u & \\frac{1}{2} \\|u - x\\|^2_2 \\\\\n",
    "    & \\text{subject to} & \\|x\\|_p = r \\\\\n",
    "    && r>0\\\\\n",
    "\\end{array}\n",
    "\n",
    "We first define the problem and constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective \n",
    "def f(r,y,x):\n",
    "    return 0.5* np.dot(y-x,y-x)\n",
    "\n",
    "# constraint\n",
    "def h(r,y,norm):\n",
    "    if norm=='L1':\n",
    "        return np.sum(np.abs(y))-r\n",
    "    if norm=='Ln':\n",
    "        return np.max(np.abs(y)) - r\n",
    "    elif norm=='L2':\n",
    "        return np.dot(y,y) - r**2\n",
    "# forward solve\n",
    "def solve_opt(x,r,f,norm):\n",
    "    result = opt.minimize(lambda y: f(r, y ,x), np.ones(np.shape(x)[0]),constraints=[{'type':'eq', 'fun': lambda y: h(r,y,norm)}] )\n",
    "    return result.x\n",
    "\n",
    "# forward solve (L2 norm only)\n",
    "def solve_analyical(x,r):\n",
    "    return r / np.sqrt(np.dot(x, x)) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the gradient for L2, L1 and Ln. We write the gradient by autograd for check too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_L2(r,x):\n",
    "    y = solve_analyical(x,r)\n",
    "    return r*y/(np.sum(y*y))\n",
    "\n",
    "def gradient_L1(r,x):\n",
    "    y= solve_opt(x,r,f,'L1')\n",
    "    a = np.sign(y)\n",
    "    return a/(a@a)\n",
    "\n",
    "# Ln in numpy is not working properly, \n",
    "# however it pytorch version is working and tested by GradCheck library.\n",
    "# please see ddn/pytorch/adaptive_projections\n",
    "def gradient_Ln(r,x):\n",
    "    y= solve_opt(x,r,f,'Ln')\n",
    "    a = np.array([0 if np.abs(yi)<np.max(np.abs(y)) else 1 for yi in y])\n",
    "    return a/(a@a)\n",
    "\n",
    "def gradient_by_auto_diff(r,x,norm):\n",
    "    fY = grad(f, 1)\n",
    "    hY = grad(h,1)\n",
    "    hR = grad(h,0)\n",
    "    frY = jacobian(fY, 0)\n",
    "    fYY = jacobian(fY, 1)\n",
    "    hYY = jacobian(hY, 1)\n",
    "    hrY= jacobian(hY, 0)\n",
    "    y= solve_opt(x,r,f,norm)\n",
    "    indx = np.nonzero(hY(r, y,norm))\n",
    "    if len(indx[0]) == 0:\n",
    "        nu= 0.0\n",
    "    nu = fY(r, y, x)[indx[0][0]] / hY(r, y, norm)[indx[0][0]]\n",
    "    H = fYY(r, y, x) - nu * hYY(r, y, norm)\n",
    "    a = hY(r, y, norm)\n",
    "    B = frY(r, y, x) - nu * hrY(r, y, norm)\n",
    "    C = hR(r, y, norm)\n",
    "    con = np.stack((a, B), axis=1)\n",
    "    try:\n",
    "        v = sci.linalg.solve(H, con, assume_a='pos')\n",
    "    except:\n",
    "         return np.full((2, 1), np.nan).squeeze()\n",
    "    return (np.outer(v[:, 0], (v[:, 0].dot(B) - C) / v[:, 0].dot(a)) - v[:, 1:1 + 1]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random generate x and r, check error between the gradient computed by different form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [-1.2419  1.18   -0.7854 -0.7418]\n",
      "r: 0.6128\n",
      "error:\n",
      "Dy(r) analytical and autograd in L2:  1.2798437478145352e-06\n",
      "Dy(r) analytical and autograd in L1:  0.0\n",
      "Dy(r) analytical and autograd in Ln:  2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/students/u6361796/.conda/envs/myconda/lib/python3.7/site-packages/autograd/tracer.py:14: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    }
   ],
   "source": [
    "x= np.array([-1.2419,  1.1800, -0.7854, -0.7418])\n",
    "#r= random.uniform(0.1, 10)\n",
    "r=np.array(0.6128)\n",
    "print('x:',x)\n",
    "print('r:',r)\n",
    "print(\"error:\")\n",
    "print(\"Dy(r) analytical and autograd in L2: \", abs(np.sum(gradient_L2(r,x)-gradient_by_auto_diff(r,x,'L2') )))\n",
    "print(\"Dy(r) analytical and autograd in L1: \", abs(np.sum(gradient_L1(r,x)-gradient_by_auto_diff(r,x,'L1') )))\n",
    "print(\"Dy(r) analytical and autograd in Ln: \", abs(np.sum(gradient_Ln(r,x)-gradient_by_auto_diff(r,x,'Ln') )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

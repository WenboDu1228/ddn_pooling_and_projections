{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import scipy.optimize as opt\n",
    "import scipy as sci\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad, jacobian\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "#from ddn.basic.learnable_robust_nodes import LearnableRobustAverage\n",
    "#from ddn.basic.robust_nodes import RobustAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find the analytic gradient of alpha in robust pooling (Dy($\\alpha$))\n",
    "I have the detail calculation available\n",
    "\n",
    "(a) pseudo-huber\n",
    "\n",
    "$y \\in  \\text{argmin}_y  \\sum_i^n \\alpha^2 (\\sqrt {1+ \\frac {(y-x_i)^2} {\\alpha^2}}-1)$\n",
    "\n",
    "\n",
    "$Dy(\\alpha) = -\\sum_{i=1}^N \\sum_{j=1}^N((\\frac {y-x_i} {\\alpha})^2 + 1)^{\\frac{3}{2}}(\\frac {(y-x_j)^3} {((\\frac {y-x_j} {\\alpha})^2 + 1)^{\\frac{3}{2}}\\alpha^3})$\n",
    "\n",
    "(b) huber\n",
    "\n",
    "$y \\in \\text{argmin}_y \\sum_{i=1}^N $ $\\begin{cases} \n",
    "\\frac {1} {2} (y-x_i)^2 & \\text{$ |y-x_i| \\leq \\alpha $} \\\\\n",
    "\\alpha(|y-x_i|-\\frac {1} {2} \\alpha) & \\text{otherwise}\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "$Dy(\\alpha) = \\sum_{i=1}^N $ $\\begin{cases} \n",
    "0 & \\text{$ |y-x_i| \\leq \\alpha $} \\\\\n",
    "1 & \\text{$ y - x_i > \\alpha $}\\\\\n",
    "-1 & \\text{$ y - x_i < \\alpha $}\n",
    "\\end{cases}$\n",
    "\n",
    "(c) welsch\n",
    "\n",
    "$y \\in  \\text{argmin}_y  \\sum_{i=1}^n (1-exp(- \\frac {(y-x_i)^2} {2\\alpha^2}))$\n",
    "\n",
    "$Dy(\\alpha) = \\sum_{i=1}^N (- \\frac {e^{-\\frac {(y-x_i)^2} {2 \\alpha^2}} (2 (y-x_i) \\alpha^2 - (y-x_i)^3)} {\\alpha^5}) $\n",
    "\n",
    "(d) Truncated Quadatic(Q: Does it make sense to train $\\alpha$ is this case)\n",
    "\n",
    "$y \\in \\text{argmin}_y \\sum_{i=1}^N $ $\\begin{cases} \n",
    "\\frac {1} {2} (y-x_i)^2 & \\text{$ |y-x_i| \\leq \\alpha $} \\\\\n",
    "\\frac {1} {2} \\alpha^2 & \\text{otherwise}\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "$Dy(\\alpha) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implment gradient on robust pooling( fix x)\n",
    "Implement analyical gradient of $\\alpha$, check the correctness by autograd gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of learnable robust pooling('pseudo-huber')\n",
    "def f(x, y, alpha,p='pseudo-huber'):\n",
    "    alpha_sq=alpha**2\n",
    "    if p=='pseudo-huber':\n",
    "        phi= lambda z: (alpha**2) * (np.sqrt(1.0 + np.power(z, 2.0) / (alpha**2)) - 1.0)\n",
    "    elif p=='huber':\n",
    "        phi = lambda z: np.where(np.abs(z) <= alpha, 0.5 * np.power(z, 2.0), alpha * np.abs(z) - 0.5 * alpha_sq)\n",
    "    elif p=='welsch':\n",
    "        phi = lambda z: 1.0 - np.exp(-0.5 * np.power(z, 2.0) / alpha_sq)\n",
    "    return np.sum([phi(y - xi) for xi in x])\n",
    "\n",
    "def solve(x,alpha ,f, p='pseudo-huber'):\n",
    "    result = opt.minimize(lambda y : f(x, y, alpha,p), np.mean(x))\n",
    "    return result.x\n",
    "\n",
    "def dyalpha_closed_form(x,y,alpha,p='pseudo-huber'):\n",
    "    alpha_sq=alpha**2\n",
    "\n",
    "    if p=='pseudo-huber':\n",
    "        dyy = np.array([np.power(1.0 + np.power(y - xi, 2.0) / alpha_sq, -1.5) for xi in x])\n",
    "        dytheta =  np.sum([np.power(y-xi,3)/(np.power(np.power((y-xi)/alpha,2)+1,1.5)*np.power(alpha,3)) for xi in x])\n",
    "    elif p=='huber':\n",
    "        dyy = np.array([1.0 if np.abs(y - xi) <= alpha else 0.0 for xi in x])\n",
    "        dytheta = np.sum(np.array([0.0 if np.abs(y - xi) <= alpha else (1.0 if y-xi>0 else -1.0) for xi in x]))\n",
    "    elif p=='welsch':\n",
    "        z = np.power(x - y, 2.0)\n",
    "        dyy = np.array([(alpha_sq - zi) / (alpha_sq * alpha_sq) * np.exp(-0.5 * zi / alpha_sq) for zi in z])\n",
    "        dytheta=np.sum(np.array([-np.exp(-0.5 * np.power((y - xi)/alpha,2))*((2*(y-xi)*alpha_sq-np.power(y-xi,3))/(alpha**5)) for xi in x])) \n",
    "    return -1.0 * dytheta/np.sum(dyy)\n",
    "    \n",
    "\n",
    "def dyalpha(x,y,alpha,p='pseudo-huber'):\n",
    "    fY = grad(f, 1)\n",
    "    fYY = jacobian(fY, 1)\n",
    "    fthetaY = jacobian(fY, 2)\n",
    "    return -1.0 * np.linalg.pinv(fYY(x, y, alpha,p)).dot(fthetaY(x, y, alpha,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "p='huber'\n",
    "n = 10 # number of input points\n",
    "y_target = np.array([0.0])\n",
    "x_init = np.random.rand(n)\n",
    "# add an outlier\n",
    "x_init[np.random.randint(len(x_init))] += 100.0 * np.random.rand(1)\n",
    "alpha_init = 1.0\n",
    "y_init = solve(x_init,alpha_init,f,p)\n",
    "# valid the analyic gradient is the same as autograd solution\n",
    "print(dyalpha_closed_form(x_init,y_init,alpha_init,p)-dyalpha(x_init,y_init,alpha_init,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euler_method(init_alpha, step_size=1/100,p='pseudo-huber'):\n",
    "    y=y_init\n",
    "    alpha = init_alpha\n",
    "    alphas=[]\n",
    "    dalphas=[]\n",
    "    for i in range(100):\n",
    "        dalpha=dyalpha_closed_form(x_init,y_init,1.0-i/100)\n",
    "        dalphas.append(dalpha)\n",
    "        if dyalpha_closed_form(x_init,y,alpha)-dyalpha(x_init,y,alpha)>1**(-15):\n",
    "            print(\"error in gradient calculation\")\n",
    "        new_alpha=alpha+step_size*dalpha\n",
    "        y=solve(x_init,new_alpha,f,p) \n",
    "        alpha=new_alpha\n",
    "        alphas.append(alpha)\n",
    "    return alphas,dalphas\n",
    "alphas,dalphas=euler_method(1.0, 1/100,p)\n",
    "#plt.figure()\n",
    "#plt.plot(range(100),alphas)\n",
    "#plt.figure()\n",
    "#plt.plot(range(100),dalphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q\n",
    "Q1: add constraint on alpha(sometimes alpha goes into negative)? eg. 1> alpha>0, so it is not unconstrainted node now.  \n",
    "\n",
    "Q2: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
    "\n",
    "Q3: case when alpha did not converage to zero(counter intuition): someone \n",
    "\n",
    "Sometimes gradient descent stuck on saddle points\n",
    "\n",
    "Sometimes it is indeed the minimum\n",
    "\n",
    "\n",
    "for Huber:\n",
    "\n",
    "[3.42539726e-02 7.72143347e-01 1.04526553e-01 9.21828824e-01\n",
    " 1.83865213e-01 8.95758725e-01 3.02897183e-01 7.75589711e-01\n",
    " 5.75906484e+01 1.98591063e-01]\n",
    " \n",
    "Pseudo-Huber: \n",
    "[ 0.76438634  0.24721799  0.64154246  0.32604462 32.22827217  0.48242317\n",
    "  0.0484155   0.92684649  0.71640308  0.54642665]\n",
    "  \n",
    "  \n",
    "Q4: convex of f($\\alpha$) and y($\\alpha$)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and modified from tutorial 2.\n",
    "def simpleGradientDescent(node, y_target, step_size=1.0e-3, tol=1.0e-7, max_iters=100, x_init=None, verbose=False):\n",
    "    \"\"\"\n",
    "    An example of gradient descent for a simple bi-level optimization problem of the form:\n",
    "        minimize_{x} 0.5 * \\|y - y_target\\|^2\n",
    "        subject to y = argmin_u f(x, u) s.t. h(x, u) = 0\n",
    "\n",
    "    Returns the solution x found and learning curve (objective function J per iteration).\n",
    "    \"\"\"\n",
    "    assert y_target.shape[0] == node.dim_y\n",
    "    x = x_init.copy() if x_init is not None else np.zeros((node.dim_x,))\n",
    "\n",
    "    J = lambda y : 0.5 * np.sum(np.square(y - y_target))\n",
    "    dJdy = lambda y : y - y_target\n",
    "    \n",
    "    theta= node.get_alpha()\n",
    "    thetas=[theta]\n",
    "    history = []\n",
    "    for i in range(max_iters):\n",
    "        # solve the lower-level problem and compute the upper-level objective\n",
    "        y, _ = node.solve(x)\n",
    "        history.append(J(y))\n",
    "        if verbose: print(\"{:5d}: {}\".format(i, history[-1]))\n",
    "        if (len(history) > 2) and (history[-2] - history[-1]) < tol:\n",
    "            break\n",
    "        # compute the gradient of the upper-level objective with respect to x via the chain rule\n",
    "        _,dytheta= node.gradient(x, y)\n",
    "        dJdtheta = (dJdy(y)* dytheta)[0]\n",
    "        theta -=step_size * dJdtheta\n",
    "        node.update_alpha(theta)\n",
    "        thetas.append(theta)\n",
    "        # take a step in the negative gradient direction\n",
    "    return x, history, thetas\n",
    "\n",
    "def lbfgs(node, y_target, max_iters=1000, x_init=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Example of using scipy.optimize to solve the problem via L-BFGS.\n",
    "    \"\"\"\n",
    "    assert y_target.shape[0] == node.dim_y\n",
    "    x_start = x_init.copy() if x_init is not None else np.zeros((node.dim_x,))\n",
    "    theta=node.get_alpha()\n",
    "    def J(theta):\n",
    "        y, _ = node.solve(x_init, theta)\n",
    "        return 0.5 * np.sum(np.square(y - y_target))\n",
    "    def dJdtheta(theta):\n",
    "        y, _ = node.solve(x_init, theta)\n",
    "        return (y - y_target)* node.gradient(x_init, y, theta)[1]\n",
    "    history = [J(theta)]\n",
    "    def callback(theta):\n",
    "        node.update_alpha(theta)\n",
    "        history.append(J(theta))\n",
    "\n",
    "    opts = {'maxiter': max_iters, 'disp': verbose}\n",
    "    result = opt.minimize(J, np.mean(x_init), args=(), method='L-BFGS-B', options=opts, callback=callback)\n",
    "    return result.x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # number of input points\n",
    "y_target = np.array([0.0])\n",
    "x_init = np.random.rand(n)\n",
    "# add an outlier\n",
    "x_init[np.random.randint(len(x_init))] += 100.0 * np.random.rand(1)\n",
    "\n",
    "\n",
    "# x_bfgs, history_bfgs = lbfgs(node, y_target, x_init=x_init)\n",
    "# print(history_bfgs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LearnableRobustAverage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-527f5e18bc33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearnableRobustAverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'huber'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_gd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimpleGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LearnableRobustAverage' is not defined"
     ]
    }
   ],
   "source": [
    "node = LearnableRobustAverage(n, penalty='huber', alpha=1.0)\n",
    "x_gd, history,thetas = simpleGradientDescent(node, y_target, step_size=0.5, x_init=x_init)\n",
    "print(x_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(thetas)\n",
    "plt.figure()\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history[-1])\n",
    "print(thetas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = lambda y : 0.5 * np.sum(np.square(y - y_target))\n",
    "nodes = [RobustAverage(n, penalty='huber', alpha=a) for a in np.linspace(0.0, 1.0, num=100)[1:]]\n",
    "solvesd= [J(n.solve(x_init)[0]) for n in nodes]\n",
    "plt.plot(solvesd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Ball projection learnable radius.\n",
    "## 1. analyical gradient using DDN theorm with equality constraint.\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "    y \\in & \\text{argmin}_u & \\frac{1}{2} \\|u - x\\|^2 \\\\\n",
    "    & \\text{subject to} & u_1^2 + u_2^2 - r^2 = 0 \\\\\n",
    "    & & \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "    A = \\text{D}_{Y} h(r,y) &= 2 u^T \\\\\n",
    "    B = \\text{D}^2_{rY} f(r, y) - \\sum_{i=1}^{3} \\lambda_i \\text{D}^2_{rY} h_i(r,y) &=0 \\\\\n",
    "    C = \\text{D}_{Y} h(r,y) &= -2r\\\\\n",
    "    2u^T \\lambda= u^T -x^T \\Rightarrow \\lambda= \\sqrt{\\frac {(u^T-x^T)(u-x)} {4^T}}\\\\ \n",
    "    H = \\text{D}^2_{YY} f(r, y) - \\sum_{i=1}^{3} \\lambda_i \\text{D}^2_{YY} h_i(r,y) &= (1 - 2 \\lambda_1) I \\\\\n",
    "    Dy(r) =  r \\cdot \\frac {y} {y^T y}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "##  2. analyical gradient of closed form.\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "    y(x,r) = \\frac {r} {\\| x\\|_2} x\\\\\n",
    "    Dy(r) = \\frac {1} {\\| x\\|_2} x\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "## 3.  gradient using DDN theorm with autograd.\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "    Dy(r) = (H^{-1} A^T(AH^{-1}A^T)^{-1}AH^{-1}B-C)-H^{-1}B\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Below we can verify three approaches gives the same solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{lll}\n",
    "    y \\in & \\text{argmin}_u & \\frac{1}{2} \\|u - x\\|^2_2 \\\\\n",
    "    & \\text{subject to} & \\|x\\|_p \\leq r \\\\\n",
    "\\end{array}\n",
    "\n",
    "\\begin{array}{lll}\n",
    "    y \\in & \\text{argmin}_u & \\frac{1}{2} \\|u - x\\|^2_2 \\\\\n",
    "    & \\text{subject to} & \\max_i |x_i| = r \\\\\n",
    "    & a = vec(sign(y))^T\\\\\n",
    "    & B = 0 \\\\\n",
    "    & H = I \\\\\n",
    "    & C = -1 \\\\\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x_init = np.random.rand(2)\n",
    "r_init = random.uniform(0.1, 10)\n",
    "x=x_init\n",
    "r=random.uniform(0.1, 10)\n",
    "def f(r,y,x):\n",
    "    return 0.5* np.dot(y-x,y-x)\n",
    "    \n",
    "def h(r,y,norm='L2'):\n",
    "    if norm=='L1':\n",
    "        return np.linalg.norm(y, 1)**2 - r**2\n",
    "    if norm=='Ln':\n",
    "        return np.linalg.norm(y, np.inf)**2 - r**2\n",
    "    else:\n",
    "        return np.dot(y,y) - r**2\n",
    "\n",
    "def solve_opt(x,r,f):\n",
    "    result = opt.minimize(lambda y: f(r, y ,x), np.array([1.0,1.0]),constraints=[{'type':'eq', 'fun': lambda y: h(r,y)}] )\n",
    "    return result.x\n",
    "\n",
    "def solve_analyical(x,r):\n",
    "    return r / np.sqrt(np.dot(x, x)) * x\n",
    "\n",
    "def gradient_closed_form(r,y,x):\n",
    "    return 1 / np.sqrt(np.dot(x, x)) * x\n",
    "\n",
    "def gradient_analyic(r,x):\n",
    "    y = solve_analyical(x,r)\n",
    "    # a = 2*y\n",
    "    # B = np.zeros(2)\n",
    "    # C = -2*r\n",
    "    # nu = np.sqrt(np.sum((y-x)**2)/(4*np.sum(y**2)))\n",
    "    # H =(1-2*nu) *np.eye(2)\n",
    "    return r*y/(np.sum(y*y))\n",
    "\n",
    "# modified from DDN basic node\n",
    "fY = grad(f, 1)\n",
    "hY = grad(h,1)\n",
    "hR = grad(h,0)\n",
    "frY = jacobian(fY, 0)\n",
    "fYY = jacobian(fY, 1)\n",
    "hYY = jacobian(hY, 1)\n",
    "hrY= jacobian(hY, 0)\n",
    "def gradient_by_auto_diff(r,x):\n",
    "    y= solve_analyical(x,r)\n",
    "    indx = np.nonzero(hY(r, y))\n",
    "    if len(indx[0]) == 0:\n",
    "        nu= 0.0\n",
    "    nu = fY(r, y, x)[indx[0][0]] / hY(r, y)[indx[0][0]]\n",
    "    H = fYY(r, y, x) - nu * hYY(r, y)\n",
    "    a = hY(r, y)\n",
    "    B = frY(r, y, x) - nu * hrY(r, y)\n",
    "    C = hR(r, y)\n",
    "    con = np.stack((a, B), axis=1)\n",
    "    try:\n",
    "        v = sci.linalg.solve(H, con, assume_a='pos')\n",
    "    except:\n",
    "         return np.full((2, 1), np.nan).squeeze()\n",
    "    return (np.outer(v[:, 0], (v[:, 0].dot(B) - C) / v[:, 0].dot(a)) - v[:, 1:1 + 1]).squeeze()\n",
    "\n",
    "y1= solve_analyical(x, r)\n",
    "print(\"forward y error betwen analyical and opt minimize\", np.abs(np.sum(y1- solve_opt(x,r,f))))\n",
    "print(\"gradient error beween closed-form and autograd:\",np.abs(np.sum(gradient_closed_form(r_init,y1,x)-gradient_by_auto_diff(r_init,x))))\n",
    "print(\"gradient error beween closed-form and DDN analyical:\",np.abs(np.sum(gradient_analyic(r_init,x)-gradient_closed_form(r_init,y1,x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed x, change radius, view y(x,r) and Dy(r).\n",
    "As we can see, the derivative is only related to x, same conclusion from closed form gradient. \n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "    y(r) = \\frac {r} {\\| x\\|_2} x\\\\\n",
    "    Dy(r) = \\frac {1} {\\| x\\|_2} x\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2)\n",
    "rs = np.linspace(0.25, 2.25, 101)\n",
    "ys = [solve_opt(x,r,f) for r in rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(rs,ys)\n",
    "plt.grid()\n",
    "plt.title(r'$y = argmin_u f(x, u)$'); plt.ylabel(r'$y$')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(rs, [gradient_by_auto_diff(r,x) for r in rs])\n",
    "#plt.plot(x, [gradient_by_ift(xi, yi) for xi, yi in zip(x, y)])\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$dy/dx$')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix radius, change x = (1, x2) , view y(x,r) and Dy(r).\n",
    "As we can see, y increases, $dy_2$ increase, $dy_1$ increase, however $\\|dy \\| $ still the same.\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "    y(r) = \\frac {r} {\\| x\\|_2} x\\\\\n",
    "    Dy(r) = \\frac {1} {\\| x\\|_2} x\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.linspace(0.25, 2.25, 101)\n",
    "x1 = np.ones(101)\n",
    "xs = np.stack((x1, x2)).T\n",
    "r = random.uniform(0.1, 10)\n",
    "ys = [solve_analyical(x,r) for x in xs]\n",
    "ys2 = [solve_opt(x,r,f) for x in xs][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(x2,ys)\n",
    "plt.grid()\n",
    "plt.legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "plt.title(r'$y = argmin_u f(x, u)$'); plt.ylabel(r'$y$')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(x2, [gradient_by_auto_diff(r,x) for x in xs])\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$dy/dx$')\n",
    "plt.legend([r\"$dy_1$\", r\"$dy_2$\"])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(x2, [np.linalg.norm(y) for y in ys])\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$dy/dx$')\n",
    "plt.legend([r\"$\\|y\\|$\",])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(x2, [np.linalg.norm(gradient_by_auto_diff(r,x)) for x in xs])\n",
    "plt.xlabel(r'$x$'); plt.ylabel(r'$dy/dx$')\n",
    "plt.legend([r\"$\\|dy\\|$\",])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare y(x,r), Dy(x,r) using different Ln norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(r,y,x):\n",
    "    return 0.5* np.dot(y-x,y-x)\n",
    "    \n",
    "def h(r,y,norm):\n",
    "    if norm=='L1':\n",
    "       # print(np.linalg.norm(y, 1)- np.sum(np.abs(y)))\n",
    "        return np.sum(np.abs(y))-r\n",
    "    if norm=='Ln':\n",
    "        # print(np.max(np.abs(y))-np.linalg.norm(y, np.inf))\n",
    "        return np.max(np.abs(y)) - r\n",
    "    elif norm=='L2':\n",
    "        return np.dot(y,y) - r**2\n",
    "def solve_opt(x,r,f,norm):\n",
    "    result = opt.minimize(lambda y: f(r, y ,x), np.array([1.0,1.0]),constraints=[{'type':'eq', 'fun': lambda y: h(r,y,norm)}] )\n",
    "    return result.x\n",
    "\n",
    "fY = grad(f, 1)\n",
    "hY = grad(h,1)\n",
    "hR = grad(h,0)\n",
    "frY = jacobian(fY, 0)\n",
    "fYY = jacobian(fY, 1)\n",
    "hYY = jacobian(hY, 1)\n",
    "hrY= jacobian(hY, 0)\n",
    "def gradient_by_auto_diff(r,x,norm):\n",
    "    y= solve_opt(x,r,f,norm)\n",
    "    #y=solve_analyical(x,r)\n",
    "    # print(y-solve_analyical(x,r))\n",
    "    indx = np.nonzero(hY(r, y,norm))\n",
    "    if len(indx[0]) == 0:\n",
    "        nu= 0.0\n",
    "    nu = fY(r, y, x)[indx[0][0]] / hY(r, y, norm)[indx[0][0]]\n",
    "    H = fYY(r, y, x) - nu * hYY(r, y, norm)\n",
    "    a = hY(r, y, norm)\n",
    "    B = frY(r, y, x) - nu * hrY(r, y, norm)\n",
    "    C = hR(r, y, norm)\n",
    "    con = np.stack((a, B), axis=1)\n",
    "    try:\n",
    "        v = sci.linalg.solve(H, con, assume_a='pos')\n",
    "    except:\n",
    "         return np.full((2, 1), np.nan).squeeze()\n",
    "    #print(nu,fYY(r, y, x),hYY(r, y, norm))\n",
    "    return (np.outer(v[:, 0], (v[:, 0].dot(B) - C) / v[:, 0].dot(a)) - v[:, 1:1 + 1]).squeeze()\n",
    "def gradient_L1(r,x):\n",
    "    y= solve_opt(x,r,f,'L1')\n",
    "    a = np.sign(y)\n",
    "    nu= (y-x)[0]*a[0]\n",
    "    B = np.zeros(2)- nu*np.zeros(2)\n",
    "    C = -1.0\n",
    "    H = np.eye(2)-nu*np.zeros(2)\n",
    "    return a/(a@a)\n",
    "\n",
    "def gradient_Ln(r,x):\n",
    "    y= solve_opt(x,r,f,'Ln')\n",
    "    a = np.array([0 if np.abs(yi)<np.max(np.abs(y)) else np.sign(yi) for yi in y])\n",
    "    idx= np.where(y==y[np.abs(y)>=np.max(np.abs(y))])\n",
    "    nu=np.sign(y)[idx]* (y-x)[idx]\n",
    "    B= np.zeros(2)- nu*np.zeros(2)\n",
    "    C= -1.0\n",
    "    H = np.eye(2)-nu*np.zeros(2)\n",
    "    return a/(a@a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.linspace(-2.25, 2.25, 101)\n",
    "x1 = np.ones(101)\n",
    "xs = np.stack((x1, x2)).T\n",
    "r = random.uniform(0.1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys1 = [solve_opt(x,r,f,'L1') for x in xs]\n",
    "ys2 = [solve_opt(x,r,f,'L2') for x in xs]\n",
    "ys3 = [solve_opt(x,r,f,'Ln') for x in xs]\n",
    "dys1 = [gradient_by_auto_diff(r,x,'L1') for x in xs]\n",
    "dys2 = [gradient_by_auto_diff(r,x,'L2') for x in xs]\n",
    "dys22 = [gradient_analyic(r,x) for x in xs]\n",
    "dys3 = [gradient_by_auto_diff(r,x,'Ln') for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(x2,ys1)\n",
    "plt.grid()\n",
    "plt.legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "plt.title(r'$y = argmin_u f(x, u)$'); plt.ylabel(r'$y$')\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(x2,ys2)\n",
    "plt.grid()\n",
    "plt.legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "plt.title(r'$y = argmin_u f(x, u)$'); plt.ylabel(r'$y$')\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(x2,ys3)\n",
    "plt.grid()\n",
    "plt.legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "plt.title(r'$y = argmin_u f(x, u)$'); plt.ylabel(r'$y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(x2,dys1)\n",
    "plt.grid()\n",
    "plt.legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "plt.title(r'$y = argmin_u f(x, u)$'); plt.ylabel(r'$y$')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(x2,dys2)\n",
    "plt.grid()\n",
    "plt.legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "plt.title(r'$y = argmin_u f(x, u)$'); plt.ylabel(r'$y$')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(x2,dys3)\n",
    "plt.grid()\n",
    "plt.legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "plt.title(r'$y = argmin_u f(x, u)$'); plt.ylabel(r'$y$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
    "# l1 ln h(r) \n",
    "# todo\n",
    "# what need to show in the tutorial\n",
    "# verify the gradient with L1, Ln norm, currently the behavior is not clear(in theory I only changed the f function)\n",
    "# further check/debug gradient descent in projection and pooling\n",
    "# train both parameter\n",
    "# possible more parameteric ddn node worth training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
